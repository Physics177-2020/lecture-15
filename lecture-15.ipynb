{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 15: Convex optimization\n",
    "\n",
    "[Convex optimization](https://en.wikipedia.org/wiki/Convex_optimization) methods are designed to find the minima of [convex functions](https://en.wikipedia.org/wiki/Convex_function). A function $f(\\underline{x})$ is convex if\n",
    "\n",
    "$$\n",
    "f(\\alpha \\underline{x} + \\beta \\underline{y}) \\leq \\alpha f(\\underline{x}) + \\beta f(\\underline{y})\\,.\n",
    "$$\n",
    "\n",
    "A function is convex if its second derivative is always non-negative (or, in the multidimensional case, if all the eigenvalues of the matrix of second derivatives are non-negative).\n",
    "\n",
    "While finding the minimum of a convex function can be challenging, it is simpler than minimizing an arbitrary function because any local minimum of a convex function is also a global minimum.\n",
    "\n",
    "\n",
    "# Steepest descent\n",
    "\n",
    "The simplest approach to minimizing a convex function is called the [steepest descent](https://en.wikipedia.org/wiki/Gradient_descent) method, sometimes also referred to as gradient descent. In this method, we start at some initial value of the parameters $\\underline{x}_0$. We then iterate through the following steps:\n",
    "\n",
    "1. Compute the direction of steepest descent $\\underline{s}$, given by the derivative $\\underline{s} = -\\nabla f(\\underline{x}_k)$\n",
    "2. Choose a distance $t$ to step along this direction\n",
    "3. Update the parameters $\\underline{x}_{k+1} = \\underline{x}_k + t \\underline{s}$\n",
    "\n",
    "This processs continues until the derivative $\\nabla f(\\underline{x})$ becomes very small, suggesting that we are close to the minimum of the function.\n",
    "\n",
    "### Example: Steepest descent optimization of a quadratic function\n",
    "\n",
    "Let's apply steepest descent in a simple example. For example, we can consider a quadratic function of a single variable\n",
    "\n",
    "$$\n",
    "f(x) = a x^2 + bx + c\\,,\n",
    "$$\n",
    "\n",
    "with $a>0$. First, we can simply plot the function and observe where the minimum lies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Define the quadratic function\n",
    "\n",
    "a =  2\n",
    "b = -1\n",
    "c =  0\n",
    "\n",
    "def f(x):\n",
    "    return a*x**2 + b*x + c\n",
    "\n",
    "\n",
    "# Plot the function\n",
    "\n",
    "x_values = np.arange(-3.5, 4, 0.01)\n",
    "y_values = f(x_values)\n",
    "\n",
    "sns.lineplot(x_values, y_values)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$f(x)$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple case we can also work out the minimum analytically. Setting the derivative \n",
    "\n",
    "$$\n",
    "\\nabla f(x) = 2ax + b = 0\\,,\n",
    "$$\n",
    "\n",
    "we find that the minimum lies at $-b/2a$. We know that this point is a minimum because the function is strictly convex. Above we chose $a=2$, $b=-1$, and $c=0$, so the minimum is located at $x^* = 1/4$.\n",
    "\n",
    "Now, let's step through the steepest descent algorithm. \n",
    "\n",
    "### Step 1. Compute the step direction\n",
    "\n",
    "In the code below we will compute the step direction for steepest descent. We'll start by defining a function for the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df(x):\n",
    "    \"\"\" Returns the derivative of the quadratic function f(x) = a x^2 + b x + c\"\"\"\n",
    "    return # FILL THIS IN\n",
    "\n",
    "\n",
    "# Choose the step direction\n",
    "\n",
    "x0 = 3  # Starting value for x\n",
    "s  =    # FILL THIS IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Choose the step length\n",
    "\n",
    "In this case, since our problem is one-dimensional the \"step direction\" is very simple -- we just need to choose whether $x$ should increase or decrease, which we determined above based on the derivative.\n",
    "\n",
    "But now that we have the direction, how far should we step? We'll talk more about choosing the step length intelligently during the next lecture. For the moment, though, let's simply choose a small $t = 0.1$ for our step size.\n",
    "\n",
    "### Step 3. Update the parameters\n",
    "\n",
    "The last step in the steepest descent algorithm is to update the parameters. We'll also check for convergence to the minimum by looking at the size of the derivative,\n",
    "\n",
    "$$\n",
    "\\left|\\nabla f(x)\\right| < \\epsilon\\,,\n",
    "$$\n",
    "\n",
    "for some suitably small value of $\\epsilon$.\n",
    "\n",
    "**Exercise**: Fill in the code below to find the minimum of $f(x)$ by steepest descent. We'll start at $x_0=3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epsilon  = 0.001  # Stopping condition -- end when |df/dx| < epsilon \n",
    "max_iter = 100    # Maximum number of iterations\n",
    "it       = 0      # Current iteration\n",
    "\n",
    "x0   = 3      # Starting value of parameter\n",
    "x    = x0     # Current value of the parameter\n",
    "dfdx = df(x0) # Starting value of the derivative df/dx\n",
    "\n",
    "# Report status\n",
    "print('iter\\tx\\tdf/dx')\n",
    "\n",
    "# Now loop through the steepest descent algorithm\n",
    "\n",
    "while np.fabs(dfdx)>=epsilon and it<max_iter:\n",
    "    \n",
    "    # Report status\n",
    "    print('%d\\t%.4f\\t%.4f' % (it, x, dfdx))\n",
    "    \n",
    "    # Choose the step direction\n",
    "    s = # FILL THIS IN\n",
    "    \n",
    "    # Choose how far to step in that direction\n",
    "    t  = 0.1\n",
    "    \n",
    "    # Update the parameters\n",
    "    x = # FILL THIS IN\n",
    "    \n",
    "    # Update the derivative\n",
    "    dfdx = # FILL THIS IN\n",
    "    \n",
    "    # Update the iteration counter\n",
    "    it += 1\n",
    "    \n",
    "# Report the minimum\n",
    "print('\\nFound the minimum near x* = %lf, true minimum is 0.25' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible issues\n",
    "\n",
    "What happens if we don't choose the step size well? Let's copy the code from above and retry it with $t=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# <Your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oh no!** These kind of traps are surprisingly easy to fall into in optimization. Next time we'll talk about methods that can avoid them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
